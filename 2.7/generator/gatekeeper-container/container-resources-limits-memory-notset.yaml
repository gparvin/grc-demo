---
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: containerresourcememory
spec:
  crd:
    spec:
      names:
        kind: ContainerResourceMemory
  targets:
    - libs:
        - |
          package lib.konstraint.core

          default is_gatekeeper = false

          is_gatekeeper {
              has_field(input, "review")
              has_field(input.review, "object")
          }

          resource = input.review.object {
              is_gatekeeper
          }

          resource = input {
              not is_gatekeeper
          }

          format(msg) = {"msg": msg} {
              true
          }

          format_with_id(msg, id) = msg_fmt {
              msg_fmt := {
                  "msg": sprintf("%s: %s", [id, msg]),
                  "details": {"policyID": id}
              }
          }

          apiVersion = resource.apiVersion
          name = resource.metadata.name
          kind = resource.kind
          labels = resource.metadata.labels
          annotations = resource.metadata.annotations
          gv := split(apiVersion, "/")
          group = gv[0] {
              contains(apiVersion, "/")
          }
          group = "core" {
              not contains(apiVersion, "/")
          }
          version := gv[count(gv) - 1]

          #parameters = input.parameters {
          #    is_gatekeeper
          #}

          #parameters = data.parameters {
          #   not is_gatekeeper
          #}

          has_field(obj, field) {
              not object.get(obj, field, "N_DEFINED") == "N_DEFINED"
          }

          missing_field(obj, field) = true {
              obj[field] == ""
          }

          missing_field(obj, field) = true {
              not has_field(obj, field)
          }
        - |-
          package lib.konstraint.pods

          import data.lib.konstraint.core

          default pod = false

          pod = core.resource.spec.template {
              pod_templates := ["daemonset","deployment","job","replicaset","replicationcontroller","statefulset"]
              lower(core.kind) == pod_templates[_]
          }

          pod = core.resource {
              lower(core.kind) == "pod"
          }

          pod = core.resource.spec.jobTemplate.spec.template {
              lower(core.kind) == "cronjob"
          }

          containers[container] {
              keys = {"containers", "initContainers"}
              all_containers = [c | keys[k]; c = pod.spec[k][_]]
              container = all_containers[_]
          }

          volumes[volume] {
              volume = pod.spec.volumes[_]
          }
        - |-
          package lib.kubernetes

          import data.lib.konstraint.core as konstraint_core

          is_deployment {
              lower(konstraint_core.apiVersion) == "apps/v1"
              lower(konstraint_core.kind) == "deployment"
          }

          is_service {
              lower(konstraint_core.apiVersion) == "v1"
              lower(konstraint_core.kind) == "service"
          }

          is_rolebinding {
              lower(konstraint_core.apiVersion) == "rbac.authorization.k8s.io/v1"
              lower(konstraint_core.kind) == "rolebinding"
          }
        - |-
          package lib.openshift

          import data.lib.konstraint.core as konstraint_core
          import data.lib.konstraint.pods as konstraint_pods
          import data.lib.kubernetes

          pod = konstraint_pods.pod {
              konstraint_pods.pod
          }

          pod = konstraint_core.resource.spec.template {
              is_deploymentconfig
          }

          containers[container] {
              keys = {"containers", "initContainers"}
              all_containers = [c | keys[k]; c = pod.spec[k][_]]
              container = all_containers[_]
          }

          is_deploymentconfig {
              lower(konstraint_core.apiVersion) == "apps.openshift.io/v1"
              lower(konstraint_core.kind) == "deploymentconfig"
          }

          is_route {
              lower(konstraint_core.apiVersion) == "route.openshift.io/v1"
              lower(konstraint_core.kind) == "route"
          }

          is_pod_or_networking {
              pod
          }

          is_pod_or_networking {
              kubernetes.is_service
          }

          is_pod_or_networking {
              is_route
          }

          is_policy_active(policyId) {
              not konstraint_core.is_gatekeeper
          }

          is_policy_active(policyId) {
              konstraint_core.is_gatekeeper

              disabledpolicies := namespace_disabled_policies_label
              not label_contains(disabledpolicies, policyId)
          }

          label_contains(disabledpolicies, policyId) {
              policyId == disabledpolicies[_]
          }

          namespace_disabled_policies_label = disabledpolicies {
              namepace := data.inventory.cluster["v1"].Namespace[konstraint_core.resource.metadata.namespace]
              label := namepace.metadata.labels["redhat-cop.github.com/gatekeeper-disabled-policies"]
              disabledpolicies := split(label, ",")
          }

          namespace_disabled_policies_label = [""] {
              namepace := data.inventory.cluster["v1"].Namespace[konstraint_core.resource.metadata.namespace]
              not namepace.metadata.labels["redhat-cop.github.com/gatekeeper-disabled-policies"]
          }
      rego: |-
        # @title RHCOP-OCP_BESTPRACT-00012: Container resource limits memory not set
        #
        # A container without a memory limit has memory utilisation of zero â€” according to the scheduler.
        # An unlimited number of Pods if schedulable on any nodes leading to resource overcommitment and potential node (and kubelet) crashes.
        # See: Resources utilisation -> https://learnk8s.io/production-best-practices#application-development
        #
        # @kinds apps.openshift.io/DeploymentConfig apps/DaemonSet apps/Deployment apps/Job apps/ReplicaSet core/ReplicationController apps/StatefulSet core/Pod batch/CronJob
        package ocp.bestpractices.container_resources_limits_memory_notset
        
        import data.lib.konstraint.core as konstraint_core
        import data.lib.openshift
        
        violation[msg] {
          #openshift.is_policy_active("RHCOP-OCP_BESTPRACT-00012")
          container := openshift.containers[_]
        
          # TODO: Maybe should use below factored out?
          #konstraint.missing_field(container.resources.limits, "memory")
          not container.resources.limits.memory
        
          msg := konstraint_core.format_with_id(sprintf("%s/%s: container '%s' has no memory limits. It is recommended to limit memory, as memory always has a maximum. See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers", [konstraint_core.kind, konstraint_core.name, container.name]), "RHCOP-OCP_BESTPRACT-00012")
        }
      target: admission.k8s.gatekeeper.sh
---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: ContainerResourceMemory
metadata:
  name: containerresourcememoryc
spec:
  enforcementAction: dryrun
  match:
    kinds:
      - apiGroups:
          - apps.openshift.io
          - apps
        kinds:
          - DeploymentConfig
          - DaemonSet
          - Deployment
          - StatefulSet
